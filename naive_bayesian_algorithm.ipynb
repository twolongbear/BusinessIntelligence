{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量： 41402\n",
      "测试集数量： 10350\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['看着 恶心 死 多人 打星 给星 好'\n '好 无聊 两 男主角 不来电 好 中国 特色 片子 一股 浓浓的 曼哈顿 下城 chinatown 杂货店 尘味' '小半截 太次' ...\n '太 妈 感人' '冗长 平淡'\n '电影 风格 适合 废柴 看 美国 拍 日剧 换 日本 演员 来演 毫无 违和感 泡 一杯 廉价 雀巢 最低 耗氧量 心跳 面 无表情 屏幕 前 呆坐 俩 小时 结尾 忘 喝咖啡 倒掉 上床睡觉 thisisit'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c068b0bc983f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m# 测试集准确率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'测试集准确率： {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;31m# 收集测试集错误\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"classes_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0;32m    726\u001b[0m                 self.class_log_prior_)\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['看着 恶心 死 多人 打星 给星 好'\n '好 无聊 两 男主角 不来电 好 中国 特色 片子 一股 浓浓的 曼哈顿 下城 chinatown 杂货店 尘味' '小半截 太次' ...\n '太 妈 感人' '冗长 平淡'\n '电影 风格 适合 废柴 看 美国 拍 日剧 换 日本 演员 来演 毫无 违和感 泡 一杯 廉价 雀巢 最低 耗氧量 心跳 面 无表情 屏幕 前 呆坐 俩 小时 结尾 忘 喝咖啡 倒掉 上床睡觉 thisisit'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "file_path = './data_set/review.csv'\n",
    "jieba.load_userdict(\"./data_set/userdict.txt\")\n",
    "stopword_path = './data_set/stopwords.txt'\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    with open(corpus_path, 'r',encoding='UTF-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = [row for row in reader]\n",
    "    # 将读取出来的语料转为list\n",
    "    review_data = np.array(rows).tolist()\n",
    "    # 打乱语料的顺序\n",
    "    random.shuffle(review_data)\n",
    "\n",
    "    review_list = []\n",
    "    sentiment_list = []\n",
    "    # 第一列为差评/好评， 第二列为评论\n",
    "    for words in review_data:\n",
    "        review_list.append(words[1])\n",
    "        sentiment_list.append(words[0])\n",
    "\n",
    "    return review_list, sentiment_list\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    with open(file_path, encoding='UTF-8') as words:\n",
    "        stop_words.extend([i.strip() for i in words.readlines()])\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def review_to_text(review):\n",
    "    stop_words = load_stopwords(stopword_path)\n",
    "    # 去除英文\n",
    "    review = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z]\", '', review)\n",
    "    review = jieba.cut(review)\n",
    "    # 去掉停用词\n",
    "    if stop_words:\n",
    "        all_stop_words = set(stop_words)\n",
    "        words = [w for w in review if w not in all_stop_words]\n",
    "    return words\n",
    "\n",
    "# 定义Pipeline对全部步骤的流式化封装和管理，可以很方便地使参数集在新数据集（比如测试集）上被重复使用。\n",
    "def MNB_Classifier():\n",
    "    return Pipeline([\n",
    "        ('count_vec', CountVectorizer()),\n",
    "        ('mnb', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "review_list, sentiment_list = load_corpus(file_path)\n",
    "\n",
    "\n",
    "n = len(review_list) // 5\n",
    "\n",
    "train_review_list, train_sentiment_list = review_list[n:], sentiment_list[n:]\n",
    "test_review_list, test_sentiment_list = review_list[:n], sentiment_list[:n]\n",
    "\n",
    "print('训练集数量： {}'.format(str(len(train_review_list))))\n",
    "print('测试集数量： {}'.format(str(len(test_review_list))))\n",
    "\n",
    "# 用于训练的评论\n",
    "review_train = [' '.join(review_to_text(review)) for review in train_review_list]\n",
    "# 对于训练评论对应的好评/差评\n",
    "sentiment_train = train_sentiment_list\n",
    "\n",
    "# 用于测试的评论\n",
    "review_test = [' '.join(review_to_text(review)) for review in test_review_list]\n",
    "# 对于测试评论对应的好评/差评\n",
    "sentiment_test = test_sentiment_list\n",
    "\n",
    "#count_vec = CountVectorizer(max_df=0.8, min_df=3)\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=3)\n",
    "#tfidf_vec = TfidfVectorizer()\n",
    "tfidftransformer = TfidfTransformer()\n",
    "# 先转换成词频矩阵，再计算TFIDF值\n",
    "tfidf = tfidftransformer.fit_transform(vectorizer.fit_transform(review_train))\n",
    "# 朴素贝叶斯中的多项式分类器\n",
    "clf = MultinomialNB().fit(tfidf, sentiment_train)\n",
    "\n",
    "#mnbc_clf = MNB_Classifier()\n",
    "\n",
    "# 进行训练\n",
    "#mnbc_clf.fit(review_train, sentiment_train)\n",
    "\n",
    "# 测试集准确率\n",
    "print('测试集准确率： {}'.format(clf.score(review_test, sentiment_test)))\n",
    "\n",
    "# 收集测试集错误\n",
    "a = mnbc_clf.predict(review_test).tolist()\n",
    "err_list = []\n",
    "for i in range(len(review_test)):\n",
    "    data = {'sentiment': '', 'review': ''}\n",
    "    if a[i] != sentiment_test[i]:\n",
    "        data['sentiment'] = sentiment_test[i]\n",
    "        data['review'] = review_test[i]\n",
    "\n",
    "        err_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "\n",
    "class SentimentAnalyzer(object):\n",
    "    def __init__(self, model_path, userdict_path, stopword_path):\n",
    "        self.clf = None\n",
    "        self.vectorizer = None\n",
    "        self.tfidftransformer = None\n",
    "        self.model_path = model_path\n",
    "        self.stopword_path = stopword_path\n",
    "        self.userdict_path = userdict_path\n",
    "        self.stop_words = []\n",
    "        self.tokenizer = jieba.Tokenizer()\n",
    "        self.initialize()\n",
    "\n",
    "    # 加载模型\n",
    "    def initialize(self):\n",
    "        with open(self.stopword_path, encoding='UTF-8') as words:\n",
    "            self.stop_words = [i.strip() for i in words.readlines()]\n",
    "\n",
    "        with open(self.model_path, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "            self.clf = model['clf']\n",
    "            self.vectorizer = model['vectorizer']\n",
    "            self.tfidftransformer = model['tfidftransformer']\n",
    "        if self.userdict_path:\n",
    "            self.tokenizer.load_userdict(self.userdict_path)\n",
    "\n",
    "    # 过滤文字中的英文与无关文字\n",
    "    def replace_text(self, text):\n",
    "        text = re.sub('((https?|ftp|file)://)?[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|].(com|cn)', '', text)\n",
    "        text = text.replace('\\u3000', '').replace('\\xa0', '').replace('”', '').replace('\"', '')\n",
    "        text = text.replace(' ', '').replace('↵', '').replace('\\n', '').replace('\\r', '').replace('\\t', '').replace('）', '')\n",
    "        text_corpus = re.split('[！。？；……;]', text)\n",
    "        return text_corpus\n",
    "\n",
    "    # 情感分析计算\n",
    "    def predict_score(self, text_corpus):\n",
    "        # 分词\n",
    "        docs = [self.__cut_word(sentence) for sentence in text_corpus]\n",
    "        new_tfidf = self.tfidftransformer.transform(self.vectorizer.transform(docs))\n",
    "        predicted = self.clf.predict_proba(new_tfidf)\n",
    "        # 四舍五入，保留三位\n",
    "        result = np.around(predicted, decimals=3)\n",
    "        return result\n",
    "\n",
    "    # jieba分词\n",
    "    def __cut_word(self, sentence):\n",
    "        words = [i for i in self.tokenizer.cut(sentence) if i not in self.stop_words]\n",
    "        result = ' '.join(words)\n",
    "        return result\n",
    "\n",
    "    def analyze(self, text):\n",
    "        text_corpus = self.replace_text(text)\n",
    "        result = self.predict_score(text_corpus)\n",
    "\n",
    "        neg = result[0][0]\n",
    "        pos = result[0][1]\n",
    "\n",
    "        print('差评： {} 好评： {}'.format(neg, pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/userdict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5cfa749566e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/userdict.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mload_userdict\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/userdict.txt'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "jieba.load_userdict(\"./data/userdict.txt\")\n",
    "\n",
    "\n",
    "file_path = './data/review.csv'\n",
    "model_export_path = './data/bayes.pkl'\n",
    "stopword_path = './data/stopwords.txt'\n",
    "\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    with open(corpus_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = [row for row in reader]\n",
    "\n",
    "    review_data = np.array(rows).tolist()\n",
    "    # 打乱数据顺序\n",
    "    random.shuffle(review_data)\n",
    "\n",
    "    review_list = []\n",
    "    sentiment_list = []\n",
    "    for words in review_data:\n",
    "        review_list.append(words[1])\n",
    "        sentiment_list.append(words[0])\n",
    "\n",
    "    return review_list, sentiment_list\n",
    "\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    with open(file_path, encoding='UTF-8') as words:\n",
    "       stop_words.extend([i.strip() for i in words.readlines()])\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "# jieba分词\n",
    "def review_to_text(review):\n",
    "    stop_words = load_stopwords(stopword_path)\n",
    "    review = jieba.cut(review)\n",
    "    all_stop_words = set(stop_words)\n",
    "    # 去掉停用词\n",
    "    review_words = [w for w in review if w not in all_stop_words]\n",
    "\n",
    "    return review_words\n",
    "\n",
    "\n",
    "review_list, sentiment_list = load_corpus(file_path)\n",
    "n = len(review_list) // 5\n",
    "\n",
    "train_review_list, train_sentiment_list = review_list[n:], sentiment_list[n:]\n",
    "test_review_list, test_sentiment_list = review_list[:n], sentiment_list[:n]\n",
    "\n",
    "print('训练集数量： {}'.format(str(len(train_review_list))))\n",
    "print('测试集数量： {}'.format(str(len(test_review_list))))\n",
    "\n",
    "review_train = [' '.join(review_to_text(review)) for review in train_review_list]\n",
    "sentiment_train = train_sentiment_list\n",
    "\n",
    "review_test = [' '.join(review_to_text(review)) for review in test_review_list]\n",
    "sentiment_test = test_sentiment_list\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=3)\n",
    "\n",
    "tfidftransformer = TfidfTransformer()\n",
    "\n",
    "# 先转换成词频矩阵，再计算TFIDF值\n",
    "tfidf = tfidftransformer.fit_transform(vectorizer.fit_transform(review_train))\n",
    "# 朴素贝叶斯中的多项式分类器\n",
    "clf = MultinomialNB().fit(tfidf, sentiment_train)\n",
    "\n",
    "# 将模型保存pickle文件\n",
    "with open(model_export_path, 'wb') as file:\n",
    "    d = {\n",
    "        \"clf\": clf,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"tfidftransformer\": tfidftransformer,\n",
    "    }\n",
    "    pickle.dump(d, file)\n",
    "\n",
    "print(\"训练完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量： 41402\n",
      "测试集数量： 10350\n",
      "训练完成\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "jieba.load_userdict(\"./data_set/userdict.txt\")\n",
    "\n",
    "\n",
    "file_path = './data_set/review.csv'\n",
    "model_export_path = './data_set/bayes.pkl'\n",
    "stopword_path = './data_set/stopwords.txt'\n",
    "\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    with open(corpus_path, 'r',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = [row for row in reader]\n",
    "\n",
    "    review_data = np.array(rows).tolist()\n",
    "    # 打乱数据顺序\n",
    "    random.shuffle(review_data)\n",
    "\n",
    "    review_list = []\n",
    "    sentiment_list = []\n",
    "    for words in review_data:\n",
    "        review_list.append(words[1])\n",
    "        sentiment_list.append(words[0])\n",
    "\n",
    "    return review_list, sentiment_list\n",
    "\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    with open(file_path, encoding='UTF-8') as words:\n",
    "        stop_words.extend([i.strip() for i in words.readlines()])\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "# jieba分词\n",
    "def review_to_text(review):\n",
    "    stop_words = load_stopwords(stopword_path)\n",
    "    review = jieba.cut(review)\n",
    "    all_stop_words = set(stop_words)\n",
    "    # 去掉停用词\n",
    "    review_words = [w for w in review if w not in all_stop_words]\n",
    "\n",
    "    return review_words\n",
    "\n",
    "\n",
    "review_list, sentiment_list = load_corpus(file_path)\n",
    "n = len(review_list) // 5\n",
    "\n",
    "train_review_list, train_sentiment_list = review_list[n:], sentiment_list[n:]\n",
    "test_review_list, test_sentiment_list = review_list[:n], sentiment_list[:n]\n",
    "\n",
    "print('训练集数量： {}'.format(str(len(train_review_list))))\n",
    "print('测试集数量： {}'.format(str(len(test_review_list))))\n",
    "\n",
    "review_train = [' '.join(review_to_text(review)) for review in train_review_list]\n",
    "sentiment_train = train_sentiment_list\n",
    "\n",
    "review_test = [' '.join(review_to_text(review)) for review in test_review_list]\n",
    "sentiment_test = test_sentiment_list\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=3)\n",
    "\n",
    "tfidftransformer = TfidfTransformer()\n",
    "\n",
    "# 先转换成词频矩阵，再计算TFIDF值\n",
    "tfidf = tfidftransformer.fit_transform(vectorizer.fit_transform(review_train))\n",
    "# 朴素贝叶斯中的多项式分类器\n",
    "clf = MultinomialNB().fit(tfidf, sentiment_train)\n",
    "\n",
    "# 将模型保存pickle文件\n",
    "with open(model_export_path, 'wb') as file:\n",
    "    d = {\n",
    "        \"clf\": clf,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"tfidftransformer\": tfidftransformer,\n",
    "    }\n",
    "    pickle.dump(d, file)\n",
    "\n",
    "print(\"训练完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\二龙熊\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.841 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "差评： 0.701 好评： 0.299\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "\n",
    "model_path = './data_set/bayes.pkl'\n",
    "userdict_path = './data_set/userdict.txt'\n",
    "stopword_path = './data_set/stopwords.txt'\n",
    "corpus_path = './data_set/review.csv'\n",
    "\n",
    "\n",
    "analyzer = SentimentAnalyzer(model_path=model_path, stopword_path=stopword_path, userdict_path=userdict_path)\n",
    "text = '加油'\n",
    "analyzer.analyze(text=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
